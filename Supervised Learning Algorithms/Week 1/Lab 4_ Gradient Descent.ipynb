{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNGmgnuM+VM2SMHGmSWj0wg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Objective\n","- Automate the process of optimizing **w** and **b** using gradient descent."],"metadata":{"id":"rAGciaNNwOh_"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"sF7Q3uvQwEK9","executionInfo":{"status":"ok","timestamp":1693558294570,"user_tz":-180,"elapsed":270,"user":{"displayName":"Mohamed Hassan","userId":"05442527335286820885"}}},"outputs":[],"source":["import math, copy\n","import numpy as np"]},{"cell_type":"markdown","source":["## Data Set\n","- You can use bigger data set, but we simplify the process."],"metadata":{"id":"rfShkFfKxWOM"}},{"cell_type":"code","source":["# Features\n","x_train = np.array([1.0, 2.0])\n","\n","# Target\n","y_train = np.array([300.0, 500.0])"],"metadata":{"id":"3SOFyYLewhIP","executionInfo":{"status":"ok","timestamp":1693558296410,"user_tz":-180,"elapsed":3,"user":{"displayName":"Mohamed Hassan","userId":"05442527335286820885"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Gradient Descent Algorithm\n","\n","- So far in this course, you have developed a linear model that predicts $f_{w,b}(x^{(i)})$:\n","$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n","- In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training you measure the cost over all of our training samples $x^{(i)},y^{(i)}$\n","$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$\n","\n","\n","- In lecture, *gradient descent* was described as:\n","\n","$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n","\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline\n"," b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n","\\end{align*}$$where, parameters $w$, $b$ are updated simultaneously.  \n","- The gradient is defined as:\n","$$\n","\\begin{align}\n","\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n","  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n","\\end{align}\n","$$\n","\n","- Here *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters."],"metadata":{"id":"vH6WBQkiyUPO"}},{"cell_type":"markdown","source":["### Steps to implement Gradient Descent\n","- You will implement gradient descent algorithm for one feature. You will need three functions.\n","  - `compute_gradient` implementing equation (4) and (5) above\n","  - `compute_cost` implementing equation (2) above (code from previous lab)\n","  - `gradient_descent`, utilizing compute_gradient and compute_cost\n","\n","- Conventions:\n","  - The naming of python variables containing partial derivatives follows this pattern,$\\frac{\\partial J(w,b)}{\\partial b}$  will be `dj_db`.\n","  - w.r.t is With Respect To, as in partial derivative of $J(wb)$ With Respect To $b$.\n"],"metadata":{"id":"aci9k_aMzby-"}},{"cell_type":"markdown","source":["1. Compute Gradient\n","\n","  `compute_gradient`  implements (4) and (5) above and returns $\\frac{\\partial J(w,b)}{\\partial w}$,$\\frac{\\partial J(w,b)}{\\partial b}$. The embedded comments describe the operations."],"metadata":{"id":"hw6AdwOf5RYV"}},{"cell_type":"code","source":["def compute_gradient(x, y, w, b):\n","    # Number of training examples\n","    m = x.shape[0]\n","\n","    # Put initial values for the derivatives terms.\n","    dj_dw = 0\n","    dj_db = 0\n","\n","    # Iterate over the examples and update the value of w and b simultainusely.\n","    for i in range(m):\n","        f_wb = w * x[i] + b\n","        dj_dw_i = (f_wb - y[i]) * x[i]\n","        dj_db_i = f_wb - y[i]\n","        dj_db += dj_db_i\n","        dj_dw += dj_dw_i\n","    dj_dw = dj_dw / m\n","    dj_db = dj_db / m\n","\n","    # Return the derivative terms.\n","    return dj_dw, dj_db"],"metadata":{"id":"qjN8-ulIy1NG","executionInfo":{"status":"ok","timestamp":1693558305427,"user_tz":-180,"elapsed":4,"user":{"displayName":"Mohamed Hassan","userId":"05442527335286820885"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["2. Compute Cost\n","\n","  Function to calculate the cost for specific value of **w** and **b**."],"metadata":{"id":"za2SuGHJx2Ft"}},{"cell_type":"code","source":["def compute_cost(x, y, w, b):\n","   # Number of examples.\n","    m = x.shape[0]\n","    cost = 0\n","\n","    # Iterate over all elements in the data set.\n","    for i in range(m):\n","        f_wb = w * x[i] + b\n","        cost = cost + (f_wb - y[i])**2\n","    total_cost = 1 / (2 * m) * cost\n","\n","    # Return the final cost.\n","    return total_cost"],"metadata":{"id":"vebMm5JvyD8u","executionInfo":{"status":"ok","timestamp":1693558308851,"user_tz":-180,"elapsed":256,"user":{"displayName":"Mohamed Hassan","userId":"05442527335286820885"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["3. Gradient Descent"],"metadata":{"id":"PNQAFo8C69b8"}},{"cell_type":"code","source":["def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function):\n","    # An array to store cost J and w's at each iteration primarily for graphing later\n","    J_history = []\n","    p_history = []\n","    b = b_in\n","    w = w_in\n","\n","    for i in range(num_iters):\n","        # Calculate the gradient and update the parameters using gradient_function\n","        dj_dw, dj_db = gradient_function(x, y, w , b)\n","\n","        # Update Parameters using equation (3) above\n","        b = b - alpha * dj_db\n","        w = w - alpha * dj_dw\n","\n","        # Save cost J at each iteration\n","        if i<100000:      # prevent resource exhaustion\n","            J_history.append( cost_function(x, y, w , b))\n","            p_history.append([w,b])\n","        # Print cost every at intervals 10 times or as many iterations if < 10\n","        if i% math.ceil(num_iters/10) == 0:\n","            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n","                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n","                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n","\n","    return w, b, J_history, p_history #return w and J,w history for graphing"],"metadata":{"id":"K27pVbsI7AYu","executionInfo":{"status":"ok","timestamp":1693558311117,"user_tz":-180,"elapsed":2,"user":{"displayName":"Mohamed Hassan","userId":"05442527335286820885"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## Put all together"],"metadata":{"id":"u6rWAsFZ7oyr"}},{"cell_type":"code","source":["# Initialize w and b.\n","w_init = 0\n","b_init = 0\n","\n","# Specify the value of Î± and the number of iterations.\n","iterations = 10000\n","tmp_alpha = 1.0e-2\n","\n","# Run Gradient Descent\n","w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha,\n","                                                    iterations, compute_cost, compute_gradient)\n","print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQSz_J7H7rBe","executionInfo":{"status":"ok","timestamp":1693558313650,"user_tz":-180,"elapsed":392,"user":{"displayName":"Mohamed Hassan","userId":"05442527335286820885"}},"outputId":"b1716e79-0b90-4b18-f914-cd3146756214"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration    0: Cost 7.93e+04  dj_dw: -6.500e+02, dj_db: -4.000e+02   w:  6.500e+00, b: 4.00000e+00\n","Iteration 1000: Cost 3.41e+00  dj_dw: -3.712e-01, dj_db:  6.007e-01   w:  1.949e+02, b: 1.08228e+02\n","Iteration 2000: Cost 7.93e-01  dj_dw: -1.789e-01, dj_db:  2.895e-01   w:  1.975e+02, b: 1.03966e+02\n","Iteration 3000: Cost 1.84e-01  dj_dw: -8.625e-02, dj_db:  1.396e-01   w:  1.988e+02, b: 1.01912e+02\n","Iteration 4000: Cost 4.28e-02  dj_dw: -4.158e-02, dj_db:  6.727e-02   w:  1.994e+02, b: 1.00922e+02\n","Iteration 5000: Cost 9.95e-03  dj_dw: -2.004e-02, dj_db:  3.243e-02   w:  1.997e+02, b: 1.00444e+02\n","Iteration 6000: Cost 2.31e-03  dj_dw: -9.660e-03, dj_db:  1.563e-02   w:  1.999e+02, b: 1.00214e+02\n","Iteration 7000: Cost 5.37e-04  dj_dw: -4.657e-03, dj_db:  7.535e-03   w:  1.999e+02, b: 1.00103e+02\n","Iteration 8000: Cost 1.25e-04  dj_dw: -2.245e-03, dj_db:  3.632e-03   w:  2.000e+02, b: 1.00050e+02\n","Iteration 9000: Cost 2.90e-05  dj_dw: -1.082e-03, dj_db:  1.751e-03   w:  2.000e+02, b: 1.00024e+02\n","(w,b) found by gradient descent: (199.9929,100.0116)\n"]}]},{"cell_type":"markdown","source":["## Make your own predictions"],"metadata":{"id":"gF7oAiaP8XaE"}},{"cell_type":"code","source":["print('Enter the size of the house please (in 1000 sqft): ')\n","user_size = float(input())\n","print(f\"{w_final*user_size + b_final:0.1f} Thousand dollars\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O3pgjfTF76Nl","executionInfo":{"status":"ok","timestamp":1693558656795,"user_tz":-180,"elapsed":2278,"user":{"displayName":"Mohamed Hassan","userId":"05442527335286820885"}},"outputId":"0d9b8354-7e22-4c95-c0be-d50d02ae9e2f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Enter the size of the house please (in 1000 sqft): \n","1.2\n","340.0 Thousand dollars\n"]}]}]}